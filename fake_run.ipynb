{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import os\n",
    "from src.data import generate_dataset\n",
    "from src.data import clear\n",
    "import pandas as pd\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(df, sample_rate, id_column):\n",
    "    return df.iloc[::sample_rate, :][id_column]\n",
    "def sample_files(raw_data_path, sample_rate, dehydrated_sample_path, id_column):\n",
    "    if not os.path.exists(dehydrated_sample_path):\n",
    "        os.makedirs(dehydrated_sample_path)\n",
    "    # find the filenames \n",
    "    file_names = sorted([name for name in os.listdir(raw_data_path) if 'dataset' in name])\n",
    "    # for every .tsv under the directory\n",
    "    for file in file_names:\n",
    "        # read the file into df\n",
    "        df = pd.read_table(f'{os.path.join(raw_data_path, file)}')\n",
    "        # sample it\n",
    "        a_sample = sample(df, sample_rate, id_column)\n",
    "        # get the saving file name {original_file_name}.txt \n",
    "        fname = file.split('.')[0] + '.txt'\n",
    "        print(f'sampling for dataset on {fname}')\n",
    "        # save the sample to the path\n",
    "        a_sample.to_csv(os.path.join(dehydrated_sample_path, fname), index = False, header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def obliviate(path):\n",
    "    for fname in os.listdir(path):\n",
    "        print(f'deleting{fname} under {path}')\n",
    "        file_path = os.path.join(path, fname)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                # ELDRITCH BLAST!!!!\n",
    "                os.remove(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                # ELDRITCH BLAST!!!!\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "\n",
    "# clear script\n",
    "def clean(paths):\n",
    "    for path in paths:\n",
    "        obliviate(path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rehydrate_tweets(raw_data_path, processed_data_path, project_path, json_data_path, sample_rate, id_column, twarc_location):\n",
    "    # Sample data and write to processed_data_path\n",
    "#     sample_files(raw_data_path, sample_rate, processed_data_path, id_column)\n",
    "    \n",
    "    # Rehydrate text file\n",
    "    if not os.path.exists(json_data_path):\n",
    "        os.makedirs(json_data_path)\n",
    "    for file in os.listdir(processed_data_path):\n",
    "        # absolute path for txt id file\n",
    "        abs_path = project_path + os.path.join(processed_data_path, file)\n",
    "        # absolute path for target directory\n",
    "        name = file.split('.')[0] + '.jsonl'\n",
    "        abs_target_path = project_path + json_data_path + name\n",
    "        print(f'saving to {abs_target_path}')\n",
    "        \n",
    "        os.system(f'{twarc_location} hydrate {abs_path} > {abs_target_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./config/data_params.json') as f:\n",
    "    data_params = json.load(f)\n",
    "with open('./config/sample_params.json') as f:\n",
    "    sample_params = json.load(f)\n",
    "\n",
    "# Cfg variables\n",
    "raw_data_path = data_params['raw_data_path']\n",
    "processed_data_path = data_params['dehydrated_data_path']\n",
    "project_path = data_params['absolute_project_path']\n",
    "twarc_path = data_params['twarc_path']\n",
    "rehydrated_json_path = data_params['rehydrated_json_path']\n",
    "rehydrated_df_path = data_params['rehydrated_df_path']\n",
    "id_column = data_params['id_column']\n",
    "from_day = data_params['from_day']\n",
    "to_day = data_params['to_day']\n",
    "want_cleaned = data_params['want_cleaned']\n",
    "\n",
    "sample_rate = sample_params['sample_every']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_names = set([name.split('.')[0] for name in os.listdir(processed_data_path)])\n",
    "json_names = set([name.split('.')[0] for name in os.listdir(rehydrated_json_path)])\n",
    "missing_names = sample_names - json_names\n",
    "sample_filename = [name + '.txt' for name in missing_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate_dataset.download_latest_datasets(raw_data_path, from_day, to_day, want_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate_dataset.rehydrate_tweets(raw_data_path, processed_data_path, project_path, rehydrated_json_path, sample_rate, id_column, twarc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
