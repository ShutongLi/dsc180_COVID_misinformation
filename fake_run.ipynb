{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import os\n",
    "from src.data import generate_dataset\n",
    "from src.data import clear\n",
    "import pandas as pd\n",
    "from twarc import Twarc\n",
    "from datetime import datetime, date, timedelta\n",
    "import run\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_twarc(api_keys_json):\n",
    "    with open(api_keys_json) as f:\n",
    "        keys = json.load(f)\n",
    "        t = Twarc(\n",
    "            keys['consumer_key'],\n",
    "            keys['consumer_secret'],\n",
    "            keys['access_token'],\n",
    "            keys['access_token_secret']\n",
    "        )\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(df, sample_rate, id_column):\n",
    "    return df.iloc[::sample_rate, :][id_column]\n",
    "def sample_files(raw_data_path, sample_rate, dehydrated_sample_path, id_column):\n",
    "    if not os.path.exists(dehydrated_sample_path):\n",
    "        os.makedirs(dehydrated_sample_path)\n",
    "    # find the filenames \n",
    "    file_names = sorted([name for name in os.listdir(raw_data_path) if 'dataset' in name])\n",
    "    # for every .tsv under the directory\n",
    "    for file in file_names:\n",
    "        # read the file into df\n",
    "        df = pd.read_table(f'{os.path.join(raw_data_path, file)}')\n",
    "        # sample it\n",
    "        a_sample = sample(df, sample_rate, id_column)\n",
    "        # get the saving file name {original_file_name}.txt \n",
    "        fname = file.split('.')[0] + '.txt'\n",
    "        print(f'sampling for dataset on {fname}')\n",
    "        # save the sample to the path\n",
    "        a_sample.to_csv(os.path.join(dehydrated_sample_path, fname), index = False, header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def obliviate(path):\n",
    "    for fname in os.listdir(path):\n",
    "        print(f'deleting{fname} under {path}')\n",
    "        file_path = os.path.join(path, fname)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                # ELDRITCH BLAST!!!!\n",
    "                os.remove(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                # ELDRITCH BLAST!!!!\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "\n",
    "# clear script\n",
    "def clean(paths):\n",
    "    for path in paths:\n",
    "        obliviate(path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_file(file_path, sample_rate, id_column):\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "    return df.iloc[::sample_rate, :][id_column]\n",
    "\n",
    "# Get the information from raw tweets we just obtained\n",
    "# processed_data_path is the path for the sampled dehydrated ids\n",
    "def rehydrate_tweets(raw_data_path, project_path, json_data_path, sample_rate, id_column, api_keys_json):\n",
    "    # Sample data and write to processed_data_path\n",
    "#     sample_files(raw_data_path, sample_rate, processed_data_path, id_column)\n",
    "    \n",
    "    t = configure_twarc(api_keys_json)\n",
    "    # Rehydrate text file\n",
    "    if not os.path.exists(json_data_path):\n",
    "        os.makedirs(json_data_path)\n",
    "        \n",
    "    sample_names = set([name.split('.')[0] for name in os.listdir(raw_data_path) if '202' in name])\n",
    "    json_names = set([name.split('.')[0] for name in os.listdir(json_data_path) if '202' in name])\n",
    "    missing_names = sample_names - json_names\n",
    "    print(f'here are the missing jsons: {missing_names}')\n",
    "        \n",
    "    for file in sorted(missing_names):\n",
    "        # absolute path for txt id file\n",
    "        abs_path = os.path.join(raw_data_path, file + '.tsv')\n",
    "        data_sample = sample_file(abs_path, sample_rate, id_column)\n",
    "        # absolute path for target directory\n",
    "        name = file + '.jsonl'\n",
    "        abs_target_path = json_data_path + name\n",
    "        print(f'saving to {abs_target_path}')\n",
    "        \n",
    "        with open(abs_target_path, 'w') as outfile:\n",
    "            for tweet in t.hydrate(data_sample):\n",
    "                outfile.write(json.dumps(tweet) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./config/data_params.json') as f:\n",
    "    data_params = json.load(f)\n",
    "with open('./config/sample_params.json') as f:\n",
    "    sample_params = json.load(f)\n",
    "\n",
    "# Cfg variables\n",
    "raw_data_path = data_params['raw_data_path']\n",
    "project_path = data_params['absolute_project_path']\n",
    "twarc_path = data_params['twarc_path']\n",
    "rehydrated_json_path = data_params['rehydrated_json_path']\n",
    "id_column = data_params['id_column']\n",
    "from_day = data_params['from_day']\n",
    "to_day = data_params['to_day']\n",
    "want_cleaned = data_params['want_cleaned']\n",
    "api_keys_path = data_params['api_keys']\n",
    "\n",
    "sample_rate = sample_params['sample_every']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here are the missing jsons: {'2020-07-08-dataset', '2020-06-10-dataset', '2020-07-14-dataset', '2020-06-28-dataset', '2020-05-25-dataset', '2020-07-26-dataset', '2020-06-19-dataset', '2020-07-24-dataset', '2020-07-03-dataset', '2020-06-29-dataset', '2020-05-26-dataset', '2020-07-11-dataset', '2020-07-16-dataset', '2020-06-24-dataset', '2020-07-18-dataset', '2020-06-25-dataset', '2020-06-08-dataset', '2020-06-22-dataset', '2020-05-31-dataset', '2020-07-01-dataset', '2020-07-22-dataset', '2020-06-27-dataset', '2020-07-25-dataset', '2020-06-18-dataset', '2020-06-01-dataset', '2020-06-02-dataset', '2020-07-06-dataset', '2020-07-29-dataset', '2020-06-12-dataset', '2020-07-15-dataset', '2020-06-05-dataset', '2020-07-07-dataset', '2020-06-04-dataset', '2020-06-09-dataset', '2020-07-04-dataset', '2020-07-12-dataset', '2020-07-27-dataset', '2020-06-20-dataset', '2020-06-13-dataset', '2020-07-17-dataset', '2020-06-23-dataset', '2020-05-29-dataset', '2020-07-21-dataset', '2020-06-06-dataset', '2020-07-31-dataset', '2020-05-30-dataset', '2020-06-26-dataset', '2020-07-19-dataset', '2020-07-10-dataset', '2020-06-16-dataset', '2020-06-11-dataset', '2020-06-14-dataset', '2020-06-17-dataset', '2020-06-21-dataset', '2020-06-03-dataset', '2020-07-05-dataset', '2020-07-09-dataset', '2020-06-15-dataset', '2020-07-30-dataset', '2020-07-28-dataset', '2020-06-07-dataset', '2020-06-30-dataset', '2020-07-02-dataset', '2020-07-20-dataset', '2020-07-23-dataset', '2020-07-13-dataset'}\n",
      "saving to data/temp/rehydration/2020-05-25-dataset.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:twarc:rate limit exceeded: sleeping 604.7220022678375 secs\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-eac7a2eaa105>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrehydrate_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrehydrated_json_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_column\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_keys_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-43-a0c3aadd6cd5>\u001b[0m in \u001b[0;36mrehydrate_tweets\u001b[0;34m(raw_data_path, project_path, json_data_path, sample_rate, id_column, api_keys_json)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs_target_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhydrate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                 \u001b[0moutfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/twarc/client.py\u001b[0m in \u001b[0;36mhydrate\u001b[0;34m(self, iterator, trim_user)\u001b[0m\n\u001b[1;32m    558\u001b[0m                     \u001b[0;34m\"include_ext_alt_text\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'true'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m                     \u001b[0;34m\"include_entities\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'true'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m                     \u001b[0;34m\"trim_user\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrim_user\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m                 })\n\u001b[1;32m    562\u001b[0m                 \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/twarc/decorators.py\u001b[0m in \u001b[0;36mnew_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m                     \u001b[0mseconds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rate limit exceeded: sleeping %s secs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseconds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseconds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rehydrate_tweets(raw_data_path, project_path, rehydrated_json_path, sample_rate, id_column, api_keys_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_names = set([name.split('.')[0] for name in os.listdir(processed_data_path)])\n",
    "json_names = set([name.split('.')[0] for name in os.listdir(rehydrated_json_path)])\n",
    "missing_names = sample_names - json_names\n",
    "sample_filename = [name + '.txt' for name in missing_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate_dataset.download_latest_datasets(raw_data_path, from_day, to_day, want_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate_dataset.rehydrate_tweets(raw_data_path, processed_data_path, project_path, rehydrated_json_path, sample_rate, id_column, twarc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
