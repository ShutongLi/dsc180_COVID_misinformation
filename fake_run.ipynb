{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import os\n",
    "from src.data import generate_dataset\n",
    "from src.data import clear\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from twarc import Twarc\n",
    "from datetime import datetime, date, timedelta\n",
    "import run\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_twarc(api_keys_json):\n",
    "    with open(api_keys_json) as f:\n",
    "        keys = json.load(f)\n",
    "        t = Twarc(\n",
    "            keys['consumer_key'],\n",
    "            keys['consumer_secret'],\n",
    "            keys['access_token'],\n",
    "            keys['access_token_secret']\n",
    "        )\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(df, sample_rate, id_column):\n",
    "    return df.iloc[::sample_rate, :][id_column]\n",
    "def sample_files(raw_data_path, sample_rate, dehydrated_sample_path, id_column):\n",
    "    if not os.path.exists(dehydrated_sample_path):\n",
    "        os.makedirs(dehydrated_sample_path)\n",
    "    # find the filenames \n",
    "    file_names = sorted([name for name in os.listdir(raw_data_path) if 'dataset' in name])\n",
    "    # for every .tsv under the directory\n",
    "    for file in file_names:\n",
    "        # read the file into df\n",
    "        df = pd.read_table(f'{os.path.join(raw_data_path, file)}')\n",
    "        # sample it\n",
    "        a_sample = sample(df, sample_rate, id_column)\n",
    "        # get the saving file name {original_file_name}.txt \n",
    "        fname = file.split('.')[0] + '.txt'\n",
    "        print(f'sampling for dataset on {fname}')\n",
    "        # save the sample to the path\n",
    "        a_sample.to_csv(os.path.join(dehydrated_sample_path, fname), index = False, header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def obliviate(path):\n",
    "    for fname in os.listdir(path):\n",
    "        print(f'deleting{fname} under {path}')\n",
    "        file_path = os.path.join(path, fname)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                # ELDRITCH BLAST!!!!\n",
    "                os.remove(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                # ELDRITCH BLAST!!!!\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "\n",
    "# clear script\n",
    "def clean(paths):\n",
    "    for path in paths:\n",
    "        obliviate(path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the information from raw tweets we just obtained\n",
    "# processed_data_path is the path for the sampled dehydrated ids\n",
    "def rehydrate_tweets(raw_data_path, processed_data_path, project_path, json_data_path, sample_rate, id_column, api_keys_json):\n",
    "    # Sample data and write to processed_data_path\n",
    "#     sample_files(raw_data_path, sample_rate, processed_data_path, id_column)\n",
    "    \n",
    "    t = configure_twarc(api_keys_json)\n",
    "    # Rehydrate text file\n",
    "    if not os.path.exists(json_data_path):\n",
    "        os.makedirs(json_data_path)\n",
    "        \n",
    "    sample_names = set([name.split('.')[0] for name in os.listdir(processed_data_path)])\n",
    "    json_names = set([name.split('.')[0] for name in os.listdir(json_data_path)])\n",
    "    missing_names = sample_names - json_names\n",
    "    print(f'here are the missing jsons: {missing_names}')\n",
    "        \n",
    "    for file in sorted(missing_names):\n",
    "        # absolute path for txt id file\n",
    "        abs_path = project_path + os.path.join(processed_data_path, file + '.txt')\n",
    "        # absolute path for target directory\n",
    "        name = file + '.jsonl'\n",
    "        abs_target_path = project_path + json_data_path + name\n",
    "        print(f'saving to {abs_target_path}')\n",
    "        \n",
    "        with open(abs_target_path, 'w') as outfile:\n",
    "            for tweet in t.hydrate(abs_path):\n",
    "                outfile.write(tweet)\n",
    "                outfile.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./config/data_params.json') as f:\n",
    "    data_params = json.load(f)\n",
    "with open('./config/sample_params.json') as f:\n",
    "    sample_params = json.load(f)\n",
    "\n",
    "# Cfg variables\n",
    "raw_data_path = data_params['raw_data_path']\n",
    "processed_data_path = data_params['dehydrated_data_path']\n",
    "project_path = data_params['absolute_project_path']\n",
    "twarc_path = data_params['twarc_path']\n",
    "rehydrated_json_path = data_params['rehydrated_json_path']\n",
    "id_column = data_params['id_column']\n",
    "from_day = data_params['from_day']\n",
    "to_day = data_params['to_day']\n",
    "want_cleaned = data_params['want_cleaned']\n",
    "api_keys_path = data_params['api_keys']\n",
    "\n",
    "sample_rate = sample_params['sample_every']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/temp/sample_by_date/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-512e1a549267>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrehydrate_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrehydrated_json_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_column\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_keys_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-69-b2fea67631d0>\u001b[0m in \u001b[0;36mrehydrate_tweets\u001b[0;34m(raw_data_path, processed_data_path, project_path, json_data_path, sample_rate, id_column, api_keys_json)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_data_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0msample_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_data_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mjson_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_data_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmissing_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_names\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mjson_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/temp/sample_by_date/'"
     ]
    }
   ],
   "source": [
    "rehydrate_tweets(raw_data_path, processed_data_path, project_path, rehydrated_json_path, sample_rate, id_column, api_keys_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_names = set([name.split('.')[0] for name in os.listdir(processed_data_path)])\n",
    "json_names = set([name.split('.')[0] for name in os.listdir(rehydrated_json_path)])\n",
    "missing_names = sample_names - json_names\n",
    "sample_filename = [name + '.txt' for name in missing_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate_dataset.download_latest_datasets(raw_data_path, from_day, to_day, want_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate_dataset.rehydrate_tweets(raw_data_path, processed_data_path, project_path, rehydrated_json_path, sample_rate, id_column, twarc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../shared_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-8bea34f827e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/datasets/home/home-03/98/098/hhliou/dsc180_project_structure/run.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Update dataset from some date (set in data_params) to today\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mgenerate_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_latest_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_day\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_day\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwant_cleaned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;31m# sample raw data of tweet ids, rehydrate them (enrich them with tweet contents)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m#     print(generate_dataset.rehydrate_tweets(raw_data_path, processed_data_path, seed, sample_size, id_column, twarc_path, day))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/datasets/home/home-03/98/098/hhliou/dsc180_project_structure/src/data/generate_dataset.py\u001b[0m in \u001b[0;36mdownload_latest_datasets\u001b[0;34m(raw_data_path, from_time, to_time, cleaned)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdownload_latest_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcleaned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# Get days of data between range\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mlast_day\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_last_day\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mdays_between\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_day\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'd'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/datasets/home/home-03/98/098/hhliou/dsc180_project_structure/src/data/generate_dataset.py\u001b[0m in \u001b[0;36mget_last_day\u001b[0;34m(raw_data_path, from_time)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mlast_day_added\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%Y-%m-%d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Get all filenames of our raw data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Just in case a file name doesn't have the date\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../shared_dir'"
     ]
    }
   ],
   "source": [
    "run.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
